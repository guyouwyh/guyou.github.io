{
    "version": "https://jsonfeed.org/version/1",
    "title": "努力走,走到灯火通明",
    "subtitle": "",
    "icon": "https://guyouwyh.github.io/guyouwyh/images/favicon.ico",
    "description": "",
    "home_page_url": "https://guyouwyh.github.io/guyouwyh",
    "items": [
        {
            "id": "https://guyouwyh.github.io/guyouwyh/2022/11/01/%E5%90%8E%E7%AB%AF/Java/Java%E5%B9%B6%E5%8F%91/JMM%E5%92%8Cvolatile%E7%9A%84%E4%B8%80%E4%BA%9B%E7%90%86%E8%A7%A3/",
            "url": "https://guyouwyh.github.io/guyouwyh/2022/11/01/%E5%90%8E%E7%AB%AF/Java/Java%E5%B9%B6%E5%8F%91/JMM%E5%92%8Cvolatile%E7%9A%84%E4%B8%80%E4%BA%9B%E7%90%86%E8%A7%A3/",
            "title": "JMM和volatile的一些理解",
            "date_published": "2022-11-01T04:10:48.805Z",
            "content_html": "<h1 id=\"jmm是什么\"><a class=\"anchor\" href=\"#jmm是什么\">#</a> JMM 是什么？</h1>\n<p>对于学 Java 多线程的人来说，一定听说过 JMM, 那么 JMM 到底是什么呢？JMM 全称 (<ins>Java Memory Model</ins>) 即 Java 内存模型。它是 Java 的一套规范，对上，是 JVM 和开发者之间的约定，对下，是 JVM 和编译器、CPU 之间的约定。</p>\n<h1 id=\"jmm是作用是什么\"><a class=\"anchor\" href=\"#jmm是作用是什么\">#</a> JMM 是作用是什么？</h1>\n<p>JMM 是作用主要是明确在多线程环境下，什么时候需要重排序，什么时候不需要重排序。为了更好的了解 JMM, 先介绍一下内存的可见性问题和重排序问题。</p>\n<h1 id=\"内存可见性问题\"><a class=\"anchor\" href=\"#内存可见性问题\">#</a> 内存可见性问题</h1>\n<p>首先来说说，为什么会存在内存可见性问题？举个栗子，假设是一个 2 核 CPU, 在 x86 架构下它的缓存布局如下:<br />\n<img data-src=\"https://raw.githubusercontent.com/guyouwyh/picture/main/20221101160705.png\" alt=\"\" /><br />\nCPU 有 3 级缓存，因为存在 CPU 缓存一致性协议 MESI, 多个 CPU 缓存不会出现不同步问题，因此不会有不可见问题。<br />\n但是还有个问题就是：使用缓存一致性协议，会对性能有很大的损耗，因此 CPU 的设计者们又进行了优化，比如在 CPU 计算单元与 L1 缓存之间又加上了 LoadBuufer 和 StoreBuffer。<br />\nL1、L2、L3 缓存与主存之间，由于有缓存一致性协议的缘故，是同步的，但是 L1 和 StoreBuffer 和 LoadBuffer 之间并不是同步的，通俗来说 ++&quot;往内存中写入一个变量，这个变量会先被写入到 StoreBuffer 中，稍后异步写入 L1 缓存中，同时同步写入主内存中&quot;++。<br />\n基于这个原因，如果我们在引入 StoreBuffer 之后，CPU 读取变量时，直接从缓存中读取，则可能出现 StoreBuffer 中存在已经修改的变量，但是还未同步到缓存中，因此 CPU 会先从 StoreBuffer 中读取，这样保证了单 CPU 顺序执行指令过程的可见性。 这种机制也被称为 Store Fowarding。</p>\n<p>但是，如果我们站在操作系统内核的角度下看 CPU 缓存模型是这样的:<br />\n<img data-src=\"https://raw.githubusercontent.com/guyouwyh/picture/main/20221101162224.png\" alt=\"\" /></p>\n<p>然后 JVM 就将这种模型抽象成了 JMM 模型:<br />\n<img data-src=\"https://raw.githubusercontent.com/guyouwyh/picture/main/20221101162419.png\" alt=\"\" /></p>\n<h1 id=\"重排序\"><a class=\"anchor\" href=\"#重排序\">#</a> 重排序</h1>\n<p>重排序由三个分类:</p>\n<ul>\n<li>编译器重排序：对于没有先后以来关系的语句，编译器可以重新调整语句的执行顺序</li>\n<li>CPU 指令重排序：在指令级别，让没有以来关系的多条指令并行</li>\n<li>CPU 内存重排序: CPU 有自己的缓存，执行的执行顺序和写入主内存的顺序不完全一致<br />\n一般而言，第三类就是造成内存可见性的主要原因。<br />\n举个例子，如果有两个线程 A 和 B。有一个全局变量 X=0, 如果线程 A 先修改了 X=1, 但是此时，由于要先写入 StoreBuffer, 此时并没有刷新到主内存，主内存中的 X 还是等于 0, 此时线程 B 看到的 X 还是 0。<br />\n将这种重排序称为内存重排序，会造成内存可见性问题。</li>\n</ul>\n<p>如果站在开发者的角度来看，肯定是希望不要有任何重排序，这样写内存的顺序也会跟代码顺序一样。<br />\n但是，如果站在 CPU 的角度来看，会尽可能的进行重排序，提升运行效率。<br />\n这时候就产生了一个问题，重排序要有什么原则？要在什么场景下进行重排序？又或者说在什么场景下不能重排序？这相当于是对于开发者和 CPU 之间的一个约定。</p>\n<h2 id=\"单线程程序的重排序规则\"><a class=\"anchor\" href=\"#单线程程序的重排序规则\">#</a> 单线程程序的重排序规则</h2>\n<p>对于单线程程序来说，只要没有产生数据依赖性，也就是操作 B 不依赖于操作 A, 那么 CPU 就可以任意重排序，因为最终产生的结果并不会改变。从开发者的角度来看，这样代码从头执行到尾，这就是 as-if-serial 语义，编译器可和 CPU 或许因为运行效率而做了重排序，但是，开发者感知不到，这样就不会产生内存可见性原因.</p>\n<h2 id=\"多线程重排序规则\"><a class=\"anchor\" href=\"#多线程重排序规则\">#</a> 多线程重排序规则</h2>\n<p>对于多线程来说，编译器和 CPU 只能保证单个线程的 as-if-serial 语义，但是，如果多个线程操作了共享变量，对于这种影响，编译期和 CPU 并不会考虑，也就产生了可见性的问题。那么为了解决在多线程的情况下的重排序问题，也就衍生出来了另外一种规则 ----<span class=\"rainbow\">happen-before</span></p>\n<p>首先介绍为什么会有 happen-before。<br />\n为了明确定义在多线程场景下，什么时候可以重排序，什么时候不能重排序，Java 引入了 JMM, 这只是一套规范，但是，如何去描述这个规范呢？JMM 就引入了 happen-before, 使用 happen-before 去描述这两个操作之间的内存可见性。</p>\n<p>那么 happen-before 究竟是什么呢？<br />\n顾名思义，happen-before 也就是发生在什么之前，如果 A happen-before B, 那么 B 应该清楚的知道 A 的所作所为，即 A 的执行结果必须对 B 可见，保证了跨线程的内存可见性。<br />\n然而，happen-before 并不代表 A 一定在 B 之前执行，但是如果 A 在 B 之前执行，那么 A 的结果一定对 B 可见，也就是定义了内存可见性的约束。<br />\nJMM 对开发者做出了一系列承诺:</p>\n<ol>\n<li>单线程中的每个操作，happen-before 对应线程中任意后续操作，即 as-if-serial 语义保证</li>\n<li>对 volatile 变量的写入，happen-before 对应后续对这个变量的读取</li>\n<li>对 synchronized 的解锁，happen-before 对应后续对这个锁的加锁。</li>\n<li>如果 A happen-before B, B happen-before C , 那么 A happen-before C (即具有传递性)</li>\n</ol>\n<p>这样也就保证了内存的可见性，但是，在操作系统底层是怎么保证的呢？这就要说说内存屏障了。</p>\n<h1 id=\"内存屏障\"><a class=\"anchor\" href=\"#内存屏障\">#</a> 内存屏障</h1>\n<p>为了禁止编译器重排序和 CPU 重排序，在编译器和 CPU 层面都有对应的指令，也就是内存屏障 (Memory Barrier). 这也正是 JMM 和 happen-before 规则的底层实现原理。</p>\n<p>编译器的内存屏障，只是为了告诉编译器不要对指令进行重排序。CPU 并不会感知编译器中内存屏障的存在</p>\n<p>而 CPU 的内存屏障是 CPU 提供的，可以由开发者显式调用。</p>\n<h2 id=\"cpu中的内存屏障\"><a class=\"anchor\" href=\"#cpu中的内存屏障\">#</a> CPU 中的内存屏障</h2>\n<p>在理论层面，可以把基本的 CPU 内存屏障分为 4 种:</p>\n<ol>\n<li>LoadLoad: 禁止读和读重排序</li>\n<li>StoreStore: 禁止写和写的重排序</li>\n<li>LoadStore: 禁止读和写的重排序</li>\n<li>StoreLoad: 禁止写和读的重排序</li>\n</ol>\n<h2 id=\"jdk中的内存屏障\"><a class=\"anchor\" href=\"#jdk中的内存屏障\">#</a> JDK 中的内存屏障</h2>\n<figure class=\"highlight java\"><figcaption data-lang=\"java\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token keyword\">public</span> <span class=\"token keyword\">native</span> <span class=\"token keyword\">void</span> <span class=\"token function\">loadFence</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre> </pre></td></tr><tr><td data-num=\"3\"></td><td><pre> <span class=\"token keyword\">public</span> <span class=\"token keyword\">native</span> <span class=\"token keyword\">void</span> <span class=\"token function\">storeFence</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"4\"></td><td><pre></pre></td></tr><tr><td data-num=\"5\"></td><td><pre> <span class=\"token keyword\">public</span> <span class=\"token keyword\">native</span> <span class=\"token keyword\">void</span> <span class=\"token function\">fullFence</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></pre></td></tr></table></figure><p>可以看到，在 Unsafe 类中，提供了三种内存屏障，这三种内存屏障与 CPU 内存屏障的对应关系如下:<br />\n<span class=\"blue\">loadFench = LoadLoad+ LoadSotre</span></p>\n<p><span class=\"blue\">storeFench = StoreStore + LoadStore</span></p>\n<p><span class=\"blue\">FullFench  = loadFence + storeFench + StoreLoad</span></p>\n<h2 id=\"内存屏障的应用-volatile\"><a class=\"anchor\" href=\"#内存屏障的应用-volatile\">#</a> 内存屏障的应用 --volatile</h2>\n<p>相信大家都知道 volatile 这个关键字。valtile 有如下作用:</p>\n<ul>\n<li>\n<p><mark>解决 64 位的写入问题</mark><br />\n JVM 的规范并没有要求 64 位的 long 或者 double 的写入是原子的。在 32 位的机器上，一个 64 位的变量写入可能被拆分成两个 32 位的写操作来执行。这样一来，读取的线程就可能读到一半的值。解决办法是在 long 前面加上 volatile 关键字。</p>\n</li>\n<li>\n<p><mark>解决内存可见性问题</mark><br />\n使用 volatile 关键字修饰的变量，会立刻从本地内存中刷新到共享内存中，保证了内存的可见性。</p>\n</li>\n<li>\n<p><mark>解决重排序问题</mark><br />\n在使用 DCL, 即双重检查锁的时候，变量需要声明为 volatle, 这里也是为了防止指令重排序。<br />\n当我们 new 一个对象的时候，其实会发生三个步骤:</p>\n</li>\n</ul>\n<ol>\n<li>在堆中开辟一段空间</li>\n<li>在这段空间 (内存) 上初始化成员变量</li>\n<li>将引用指向这段空间的地址</li>\n</ol>\n<p>如果将步骤 2 和步骤 3 重排序了，获取的对象将会是未初始化的对象。这就会造成一些很严重的问题。</p>\n<p>事实上，volatile 关键字底层就是使用了内存屏障。<br />\n<strong>1. 在 volatile 写操作前面插入一个 StoreStore 屏障。保证 volatile 写操作不会和之前的写操作重排序<br />\n 2. 在 volatile 写操作后插入一个 StoreLoad 屏障，保证 volatile 写操作不会和之后的读操作重排序<br />\n 3. 在 volatile 读操作后面插入一个 LoadLoad 屏障和 LoadStore 屏障，保证 volatile 读操作不会和之后的读操作、写操作重排序</strong></p>\n<h1 id=\"总结\"><a class=\"anchor\" href=\"#总结\">#</a> 总结</h1>\n<p><strong>通俗来说，JMM 其实就是 Java 提供的一种内存模型，每个线程都有自己的本地缓存，所有线程都有一个共享内存，本地缓存中的变量相当于是共享内存中变量的一个副本，当我们修改一个变量的时候，事实上会直接修改本地缓存中的变量，并不会立即刷新到共享内存中，这样就导致了多个线程之间不可见问题。<br />\n如果站在我们开发者的角度来说，可以使用 synchronized、volatile、final 去解决重排序以及可见性问题。<br />\n但是如果站在 JVM 的角度来说，它其实是通过 JMM 的 happen-before 规则来解决的。<br />\n更深层次一点，站在 CPU 层面来说，主要是通过内存屏障来解决的。</strong></p>\n",
            "tags": [
                "后端",
                "Java",
                "Java并发",
                "Java"
            ]
        }
    ]
}